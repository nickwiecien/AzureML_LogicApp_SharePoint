{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f11c844",
   "metadata": {},
   "source": [
    "## Azure Machine Learning - Data Processing Pipeline\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load and process data from an AML-linked blob storage account that has moved from SharePoint via a Logic App.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23cb83",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf498ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf95d1",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19a41d",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "\n",
    "Here, we also register the environment to the AML workspace so that it can be used for future retraining and inferencing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment(name='sample_env')\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\n",
    "run_config.environment.python.conda_dependencies.set_pip_requirements([\n",
    "    'requests',\n",
    "    'scikit-learn==0.22.1',\n",
    "    'pandas',\n",
    "    'joblib',\n",
    "    'openpyxl==3.0.9',\n",
    "    'azureml-core',\n",
    "    'azureml-automl-core',\n",
    "    'azureml-train-automl',\n",
    "    'azureml-train-core',\n",
    "    'azureml-automl-runtime',\n",
    "    'azureml-train-automl-client',\n",
    "    'azureml-train-automl-runtime'\n",
    "])\n",
    "# run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\n",
    "\n",
    "#Register environment for reuse \n",
    "run_config.environment.register(ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5306c27b",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import DataType\n",
    "column_dictionary = {\n",
    "    'WeekStarting':DataType.to_datetime(),\n",
    "    'Store':DataType.to_string(),\n",
    "    'Brand':DataType.to_string(),\n",
    "    'Quantity':DataType.to_long(),\n",
    "    'Advert':DataType.to_bool(),\n",
    "    'Price':DataType.to_float(),\n",
    "    'Revenue':DataType.to_float()\n",
    "}\n",
    "\n",
    "input_data = OutputFileDatasetConfig(name='Input_Data', destination=(default_ds, 'input_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='Input_Data')\n",
    "training_data = OutputFileDatasetConfig(name='Training_Data', destination=(default_ds, 'training_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='Training_Data')\n",
    "forecasting_data = OutputFileDatasetConfig(name='Forecasting_Data', destination=(default_ds, 'forecasting_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='Forecasting_Data')\n",
    "forecast_results_data = OutputFileDatasetConfig(name='Forecast_Results_Data', destination=(default_ds, 'forecast_results_data/{run-id}')).read_delimited_files(set_column_types=column_dictionary).register_on_complete(name='Forecast_Results_Data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df954c13",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify the path to the data added to the default datastore via a Logic App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = PipelineParameter(name='file_path', default_value='sharepoint_data/OJData.csv')\n",
    "model_name = PipelineParameter(name='model_name', default_value='FORECASTING_MODEL')\n",
    "target_column = PipelineParameter(name='target_column', default_value='Quantity')\n",
    "timestamp_column = PipelineParameter(name='timestamp_column', default_value='WeekStarting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19742a8b",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of a single step which represents how to load data from the default datastore, register as an input dataset, apply some transformations, then register as an output dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and process raw data from AML-linked datastore\n",
    "# Register tabular dataset after retrieval\n",
    "get_and_split_step = PythonScriptStep(\n",
    "    name='Get Data and Split',\n",
    "    script_name='get_and_split_data.py',\n",
    "    arguments =['--input_data', input_data,\n",
    "                '--training_data', training_data,\n",
    "                '--forecasting_data', forecasting_data,\n",
    "                '--timestamp_column', timestamp_column,\n",
    "                '--file_path', file_path],\n",
    "    outputs=[training_data, forecasting_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=True,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 90,\n",
    "    \"iterations\" : 5,\n",
    "    \"experiment_timeout_hours\" : 6,\n",
    "    \"primary_metric\" : 'normalized_root_mean_squared_error',\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'forecasting',\n",
    "                             path = '.',\n",
    "                             debug_log = 'automated_ml_errors.log',\n",
    "                             compute_target = cpu_cluster,\n",
    "                             run_configuration = run_config,\n",
    "                             featurization = 'auto',\n",
    "                             training_data = training_data,\n",
    "                             label_column_name = 'Quantity',\n",
    "                             max_cores_per_iteration = -1,\n",
    "                             max_concurrent_iterations = 3,\n",
    "                             time_column_name = 'WeekStarting',\n",
    "                             max_horizon = 10,\n",
    "                             **automl_settings)\n",
    "\n",
    "train_model_step = AutoMLStep(name='Train Forecasting Model (AutoML)',\n",
    "    automl_config=automl_config,\n",
    "    passthru_automl_config=False,\n",
    "    enable_default_model_output=False,\n",
    "    enable_default_metrics_output=False,\n",
    "    allow_reuse=True)\n",
    "\n",
    "train_model_step.run_after(get_and_split_step)\n",
    "\n",
    "generate_forecast_step = PythonScriptStep(\n",
    "    name='Register Model and Generate Forecast',\n",
    "    script_name='generate_forecast.py',\n",
    "    arguments =['--model_name', model_name,\n",
    "                '--target_column', target_column,\n",
    "                '--forecast_results_data', forecast_results_data],\n",
    "    inputs=[forecasting_data.as_input(name='Forecast_Data')],\n",
    "    outputs=[forecast_results_data],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "generate_forecast_step.run_after(train_model_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33ec2b",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af25abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_and_split_step, train_model_step, generate_forecast_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ecc112",
   "metadata": {},
   "source": [
    "### Optional: Trigger a Pipeline Execution from the Notebook\n",
    "You can create an Experiment (logical collection for runs) and submit a pipeline run directly from this notebook by running the commands below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9400986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(ws, 'sample-pipeline-run')\n",
    "# run = experiment.submit(pipeline)\n",
    "# run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88627d9a",
   "metadata": {},
   "source": [
    "### Create a Published PipelineEndpoint\n",
    "Once we have created our pipeline we will look to retrain our model periodically as new data becomes available. By publishing our pipeline to a `PipelineEndpoint` we can iterate on our pipeline definition but maintain a consistent REST API endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bea386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=\"Endpoint to my pipeline\",\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_endpoint_name = 'AML Pipeline Endpoint'\n",
    "pipeline_endpoint_description = 'Sample pipeline for loading data from an AML datastore, processing, and storing results'\n",
    "\n",
    "published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                     description=pipeline_endpoint_description,\n",
    "                                     continue_on_step_failure=False)\n",
    "\n",
    "published_pipeline_to_pipeline_endpoint(\n",
    "    workspace=ws,\n",
    "    published_pipeline=published_pipeline,\n",
    "    pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=pipeline_endpoint_description\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4dd472",
   "metadata": {},
   "source": [
    "### Sample Pipeline Trigger Request (REST API)\n",
    "You can trigger your published pipeline remotely by making an authenticated call the PipelineEndpoint's REST API. The sample request code below requires creation of a service principal and assignment of that SP to your AML workspace as a Contributor [more details here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication). When triggering a pipeline using a REST API call you are required to provide an Experiment Name and can optionally updated the default pipeline parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97722c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "pipeline_endpoint = os.environ.get('pipeline_endpoint')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(pipeline_endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Pipeline_Trigger_Test\", \"ParameterAssignments\": {\"testing_size\": 0.3}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
